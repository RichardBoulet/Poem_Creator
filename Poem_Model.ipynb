{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poem Generation Using Web-Scraped Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The goal of this project is to generate (decent) poems from scraped website text using a keyowrd as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.utils as ku \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "text = open('poem.txt',encoding=\"utf8\").read()\n",
    "\n",
    "# Split threats each line as a sentence.\n",
    "corpus = text.lower().split(\"\\n\")\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "\n",
    "input_sequences = []\n",
    "\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    \n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i + 1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "        \n",
    "# Pad sequences\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen = max_sequence_len, padding='pre'))\n",
    "\n",
    "\n",
    "# Create predictors and label\n",
    "predictors, label = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "\n",
    "label = ku.to_categorical(label, num_classes = total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 152, 100)          416100    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 152, 300)          301200    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 152, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2080)              210080    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4161)              8659041   \n",
      "=================================================================\n",
      "Total params: 9,746,821\n",
      "Trainable params: 9,746,821\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "571/571 [==============================] - 19s 33ms/step - loss: 7.0924 - accuracy: 0.0698\n",
      "Epoch 2/100\n",
      "571/571 [==============================] - 19s 34ms/step - loss: 6.5236 - accuracy: 0.0713\n",
      "Epoch 3/100\n",
      "571/571 [==============================] - 19s 33ms/step - loss: 6.3136 - accuracy: 0.0867\n",
      "Epoch 4/100\n",
      "571/571 [==============================] - 19s 34ms/step - loss: 6.1629 - accuracy: 0.0936\n",
      "Epoch 5/100\n",
      "571/571 [==============================] - 19s 34ms/step - loss: 5.9840 - accuracy: 0.1005\n",
      "Epoch 6/100\n",
      "571/571 [==============================] - 19s 33ms/step - loss: 5.8031 - accuracy: 0.1085\n",
      "Epoch 7/100\n",
      "571/571 [==============================] - 19s 34ms/step - loss: 5.6510 - accuracy: 0.1161\n",
      "Epoch 8/100\n",
      "571/571 [==============================] - 19s 33ms/step - loss: 5.5278 - accuracy: 0.1209\n",
      "Epoch 9/100\n",
      "571/571 [==============================] - 19s 33ms/step - loss: 5.4199 - accuracy: 0.1258\n",
      "Epoch 10/100\n",
      "571/571 [==============================] - 19s 33ms/step - loss: 5.3209 - accuracy: 0.1302\n",
      "Epoch 11/100\n",
      "571/571 [==============================] - 19s 33ms/step - loss: 5.2309 - accuracy: 0.1329\n",
      "Epoch 12/100\n",
      "571/571 [==============================] - 19s 33ms/step - loss: 5.1406 - accuracy: 0.1382\n",
      "Epoch 13/100\n",
      "571/571 [==============================] - 18s 32ms/step - loss: 5.0578 - accuracy: 0.1428\n",
      "Epoch 14/100\n",
      "571/571 [==============================] - 18s 32ms/step - loss: 4.9758 - accuracy: 0.1503\n",
      "Epoch 15/100\n",
      "571/571 [==============================] - 18s 31ms/step - loss: 4.8974 - accuracy: 0.1581\n",
      "Epoch 16/100\n",
      "571/571 [==============================] - 18s 32ms/step - loss: 4.8156 - accuracy: 0.1653\n",
      "Epoch 17/100\n",
      "571/571 [==============================] - 18s 31ms/step - loss: 4.7292 - accuracy: 0.1738\n",
      "Epoch 18/100\n",
      "571/571 [==============================] - 18s 31ms/step - loss: 4.6597 - accuracy: 0.1769\n",
      "Epoch 19/100\n",
      "571/571 [==============================] - 18s 31ms/step - loss: 4.5764 - accuracy: 0.1837\n",
      "Epoch 20/100\n",
      "571/571 [==============================] - 18s 31ms/step - loss: 4.4921 - accuracy: 0.1949\n",
      "Epoch 21/100\n",
      "571/571 [==============================] - 18s 31ms/step - loss: 4.4153 - accuracy: 0.2042\n",
      "Epoch 22/100\n",
      "571/571 [==============================] - 18s 31ms/step - loss: 4.3389 - accuracy: 0.2114\n",
      "Epoch 23/100\n",
      "571/571 [==============================] - 18s 31ms/step - loss: 4.2574 - accuracy: 0.2215\n",
      "Epoch 24/100\n",
      "571/571 [==============================] - 17s 31ms/step - loss: 4.1832 - accuracy: 0.2318\n",
      "Epoch 25/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 4.1112 - accuracy: 0.2417\n",
      "Epoch 26/100\n",
      "571/571 [==============================] - 17s 31ms/step - loss: 4.0365 - accuracy: 0.2516\n",
      "Epoch 27/100\n",
      "571/571 [==============================] - 17s 31ms/step - loss: 3.9608 - accuracy: 0.2611\n",
      "Epoch 28/100\n",
      "571/571 [==============================] - 17s 31ms/step - loss: 3.8887 - accuracy: 0.2715\n",
      "Epoch 29/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 3.8238 - accuracy: 0.2834\n",
      "Epoch 30/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 3.7570 - accuracy: 0.2909\n",
      "Epoch 31/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 3.6896 - accuracy: 0.3044\n",
      "Epoch 32/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 3.6150 - accuracy: 0.3144\n",
      "Epoch 33/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 3.5531 - accuracy: 0.3281\n",
      "Epoch 34/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 3.4838 - accuracy: 0.3383\n",
      "Epoch 35/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 3.4225 - accuracy: 0.3494\n",
      "Epoch 36/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 3.3649 - accuracy: 0.3628\n",
      "Epoch 37/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 3.3003 - accuracy: 0.3733\n",
      "Epoch 38/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 3.2358 - accuracy: 0.3856\n",
      "Epoch 39/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 3.1793 - accuracy: 0.3994\n",
      "Epoch 40/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 3.1194 - accuracy: 0.4136\n",
      "Epoch 41/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 3.0687 - accuracy: 0.4242\n",
      "Epoch 42/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 3.0145 - accuracy: 0.4336\n",
      "Epoch 43/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.9575 - accuracy: 0.4418\n",
      "Epoch 44/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.9133 - accuracy: 0.4544\n",
      "Epoch 45/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.8676 - accuracy: 0.4595\n",
      "Epoch 46/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.8131 - accuracy: 0.4712\n",
      "Epoch 47/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.7570 - accuracy: 0.4874\n",
      "Epoch 48/100\n",
      "571/571 [==============================] - 17s 29ms/step - loss: 2.7166 - accuracy: 0.4941\n",
      "Epoch 49/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.6757 - accuracy: 0.5010\n",
      "Epoch 50/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.6261 - accuracy: 0.5139\n",
      "Epoch 51/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.5891 - accuracy: 0.5199\n",
      "Epoch 52/100\n",
      "571/571 [==============================] - 17s 29ms/step - loss: 2.5401 - accuracy: 0.5286\n",
      "Epoch 53/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.5042 - accuracy: 0.5365\n",
      "Epoch 54/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.4654 - accuracy: 0.5443\n",
      "Epoch 55/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.4277 - accuracy: 0.5505\n",
      "Epoch 56/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.3939 - accuracy: 0.5582\n",
      "Epoch 57/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.3562 - accuracy: 0.5663\n",
      "Epoch 58/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.3173 - accuracy: 0.5738\n",
      "Epoch 59/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.3047 - accuracy: 0.5786\n",
      "Epoch 60/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.2550 - accuracy: 0.5892\n",
      "Epoch 61/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.2258 - accuracy: 0.5944\n",
      "Epoch 62/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.1943 - accuracy: 0.5987\n",
      "Epoch 63/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.1609 - accuracy: 0.6050\n",
      "Epoch 64/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.1375 - accuracy: 0.6102\n",
      "Epoch 65/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.1039 - accuracy: 0.6187\n",
      "Epoch 66/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.0811 - accuracy: 0.6234\n",
      "Epoch 67/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.0524 - accuracy: 0.6239\n",
      "Epoch 68/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 2.0248 - accuracy: 0.6338\n",
      "Epoch 69/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.9890 - accuracy: 0.6419\n",
      "Epoch 70/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.9620 - accuracy: 0.6473\n",
      "Epoch 71/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.9430 - accuracy: 0.6503\n",
      "Epoch 72/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.9265 - accuracy: 0.6549\n",
      "Epoch 73/100\n",
      "571/571 [==============================] - 17s 29ms/step - loss: 1.8966 - accuracy: 0.6593\n",
      "Epoch 74/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.8832 - accuracy: 0.6596\n",
      "Epoch 75/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.8507 - accuracy: 0.6664\n",
      "Epoch 76/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.8335 - accuracy: 0.6698\n",
      "Epoch 77/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.8138 - accuracy: 0.6735\n",
      "Epoch 78/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.7872 - accuracy: 0.6793\n",
      "Epoch 79/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.7775 - accuracy: 0.6829\n",
      "Epoch 80/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.7534 - accuracy: 0.6864\n",
      "Epoch 81/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.7415 - accuracy: 0.6854\n",
      "Epoch 82/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.7176 - accuracy: 0.6910\n",
      "Epoch 83/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.6922 - accuracy: 0.6965\n",
      "Epoch 84/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.6761 - accuracy: 0.7010\n",
      "Epoch 85/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.6665 - accuracy: 0.7022\n",
      "Epoch 86/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.6527 - accuracy: 0.7041\n",
      "Epoch 87/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.6318 - accuracy: 0.7079\n",
      "Epoch 88/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.6195 - accuracy: 0.7104\n",
      "Epoch 89/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.5897 - accuracy: 0.7173\n",
      "Epoch 90/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.5812 - accuracy: 0.7151\n",
      "Epoch 91/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.5762 - accuracy: 0.7196\n",
      "Epoch 92/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.5460 - accuracy: 0.7245\n",
      "Epoch 93/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.5350 - accuracy: 0.7261\n",
      "Epoch 94/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.5238 - accuracy: 0.7289\n",
      "Epoch 95/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.5675 - accuracy: 0.7137\n",
      "Epoch 96/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.5173 - accuracy: 0.7280\n",
      "Epoch 97/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.4738 - accuracy: 0.7347\n",
      "Epoch 98/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.4663 - accuracy: 0.7370\n",
      "Epoch 99/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.4435 - accuracy: 0.7407\n",
      "Epoch 100/100\n",
      "571/571 [==============================] - 17s 30ms/step - loss: 1.4441 - accuracy: 0.7412\n"
     ]
    }
   ],
   "source": [
    "#taking long time given epoch count, need to look into this and adjusting for repetitive word use in outputs when trained with only 10 epochs\n",
    "\n",
    "history = model.fit(predictors, label, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world it deaf and macbeth john purgatory satirical assay of the heather attached hollows of sonnets said he came and tulips by sylvia plath two remarkable poems by the golden haired rock star of american poetry they had left the lady of shallot by alfred lord tennyson rime of the night\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"world\"\n",
    "next_words = 50\n",
    "  \n",
    "for _ in range(next_words):\n",
    " token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    " token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    " predicted = model.predict_classes(token_list, verbose=0)\n",
    " output_word = \"\"\n",
    " for word, index in tokenizer.word_index.items():\n",
    "  if index == predicted:\n",
    "   output_word = word\n",
    "   break\n",
    " seed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poem",
   "language": "python",
   "name": "poem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
